This chapter focuses on describing the main technicalities of machine learning algorithms that are commonly used in RS and shall be tested in this research. Sparsity concepts are also presented in order to establish key definitions that support the proposed framework to evaluate sparsity effects on recommendation quality.

\section{Neighborhood Models}

As aforementioned in Section \ref{sec:CF}, neighborhood-based models use the principle of \textit{word-of-mouth} where the feedbacks of like-minded users are employed to compute item preferences on a cluster of peers. Since similarity between two entities are at the core of these models, two main approaches are described: the item- and user-based clustering.

User-based approach was applied in the early CF techniques, where users similarities were computed to estimate feedback prediction \cite{1999AlgorithmicFramework}. However, not only does user-user similarity matrices frequently present high dimensionality, but also does not provide helpful explanations on recommendations. To overcome these drawbacks, item-item similarity matrix is used instead, since the fact that usually $|\mathcal{U}| < |\mathcal{I}|$ results in a lower dimension similarity matrix \cite{2001sarwar}. With this perspective, the item-based clustering method is formally described as follows.

Given the set $\mathcal{N}_i(u)$ of $k$ most similar users to $u$ who have interacted with item $i$, $r_{ui}$ can be estimated according to Equation \ref{eq:itemknn}

\begin{equation} 
    \label{eq:itemknn}
    \hat{r}_{ui} = b_{ui} + \frac{\sum_{v \in \mathcal{N}_i(u)} \rho(u,v)\cdot (r_{vi}-b_{vi})}{\sum_{v \in \mathcal{N}_i(u)} \rho(u,v)}
\end{equation} where $\rho(u,v)$ is a distance or similarity measure between users $u$ and $v$. Several  measures have been used in this scenario, such as the Pearson correlation coefficient, the cosine similarity or the euclidean distance \cite{2010Handbook, 10.1145/3133264.3133299}.

%% Trocar similarity por distancia e usar outras distancias - Euclidean, Minkiwski, Mahalanobis, Cosine

\section{Latent Factor Models}


\subsection{Matrix Factorization}

    Matrix factorization (MF) \abbrev{MF}{Matrix Factorization} models map users and items to a joint latent factor space of dimensionality $f$ such that user-item interactions are modeled as inner products in that space. Formally, the utility matrix $\mathbf{R}$ can be modeled as Equation \ref{eq:MF}
    
    \begin{equation}
        \label{eq:MF}
        \mathbf{R_{|\mathcal{U}| \times |\mathcal{I}|}} = \mathbf{Q}_{|\mathcal{I}| \times f} \mathbf{P}_{f \times |\mathcal{U}|} 
    \end{equation} where each item $i$ and user $u$ are mapped as vectors $\mathbf{q}_i \in \mathbb{R}^f$ and $\mathbf{p}_u \in \mathbb{R}^f$, respectively, and their interaction is given by $r_{ui} = \mathbf{q}^T_i \cdot \mathbf{p}_u$ \cite{2009MFTechniques}.
    
    The goal of the learning algorithm is to estimate the parameters of  $\mathbf{q}_i$ and $\mathbf{p}_u$. However, since most of the user-item interactions in $\mathbf{R}$ are unknown, the algorithm can only take into account the error associated with known entries of the matrix. Furthermore, in this set there are $(|\mathcal{U}|+|\mathcal{I}|)\cdot f$ parameters to be found using at most $|\mathcal{U}|\cdot |\mathcal{I}|$ interactions, which is hardly the usual scenario since users most often interact with a small number of items. As a consequence, the learning algorithm needs to find a great number of parameters using few training samples, making over-fitting a problem to be dealt with \cite{2008ALSWR}.
    
    Mathematically, let $\mathcal{K}$ be the set of $(u,i)$ pairs for which $r_{ui}$ is known. Then, the learning algorithm is designed to solve the following optimization problem: 
    
    \begin{equation}
        \label{eq:mf_min}
        \min_{\mathbf{q}^*, \mathbf{p}^*} \sum_{(u,i) \in \mathcal{K}} \bigg(r_{ui} - \mathbf{q}^T_i \cdot \mathbf{p}_u\bigg)^2 + \lambda \bigg(\sum_{i \in \mathcal{I}^+}||\mathbf{q}_i||^2 + \sum_{u \in \mathcal{U}^+}||\mathbf{p}_u||^2 \bigg)^2
    \end{equation} where $\lambda$ is a regularization parameter that can be estimated through cross-validation (probabilistic estimation is also proposed in \cite{2007ProbMF}). Two approaches are widely used to solve this optimization problem: the stochastic gradient descent and the alternating least squares. 
    
    By using Stochastic Gradient Descent (SGD) \abbrev{SGD}{Stochastic Gradient Descent} [Ref Simon Funk], the algorithm loops through all user-item pairs in a training set and computes an associated prediction error $e_{ui} \triangleq r_{ui} - \mathbf{q}^T_i \cdot \mathbf{p}_u$, updating the parameters by a magnitude proportional to a learning rate $\alpha$ as shown in Equation \ref{eq:sgd}.
    
    \begin{equation}
        \label{eq:sgd}
            \begin{cases}
            \mathbf{q}_i \leftarrow \mathbf{q}_i + \alpha (e_{ui}\cdot \mathbf{p}_u-\lambda \cdot \mathbf{q}_i) \\
            
            \mathbf{p}_u \leftarrow \mathbf{p}_u + \alpha (e_{ui}\cdot \mathbf{q}_i-\lambda \cdot \mathbf{p}_u)
            \end{cases}
    \end{equation} Although being simpler to implement, this approach is not suitable for parallelization since the learning algorithm needs to loop over all training set several times. This is a major drawback since most RS data sets have massive amount of data. In order to use the computational advantages of distributed systems, the Alternating Least Squares (ALS) \abbrev{ALS}{Alternating Least Squares} approach has been proposed by considering the minimization problem as follows.

    %PCA / SCA / ICA
    
    Due to the fact that both $\mathbf{q}_i$ and $\mathbf{p}_u$ are variables, Equation \ref{eq:mf_min} is not convex, leading to possible local minimum. On the other hand, if one of these variables are fixed, then the problem becomes quadratic and thus can be solved optimally. Consequently, the ALS switches between considering one variable fixed and solving the problem to the other, and then doing the opposite in a second step. The training steps can be summarized as follows \cite{2008ALSWR}:
    
    \begin{itemize}
        \item Step 1: Initialize matrix $\mathbf{P}$;
        \item Step 2: Fix $\mathbf{P}$ and solve $\mathbf{Q}$ by minimizing the objective function;
        \item Step 3: Fix $\mathbf{Q}$ and solve $\mathbf{P}$ by minimizing the objective function similarly;
        \item Step 4: Repeat Steps 2 and 3 until a stopping criterion is satisfied.
    \end{itemize}
    
    Additionally, weighted-$\lambda$-regularized (ALS-WR) is proposed in \cite{2008ALSWR} by modifying the regularization term as in Equation \ref{eq:mf_min_weighted}
    
    \begin{equation}
        \label{eq:mf_min_weighted}
        \min_{\mathbf{q}^*, \mathbf{p}^*} \sum_{(u,i) \in \mathcal{K}} (r_{ui} - \mathbf{q}^T_i \cdot \mathbf{p}_u)^2 + \lambda \Big(\sum_i n(i)||\mathbf{q}_i||^2 + \sum_u n(u)||\mathbf{p}_u||\Big)^2
    \end{equation} where $n(u)$ and $n(i)$ are the total number of interactions of user $u$ and item $i$, respectively.

\subsection{Auto-encoders}


\section{Sparsity}

    Since sparsity is at the core of this research, its proper definition needs to be layed out. However, several sparsity concepts and measures have been proposed in the literature. This research focuses on the concept presented by Hurley et al. \cite{10.1109/TIT.2009.2027527}, which is defined as: 
    
    \begin{quote}
        \textit{A sparse representation is one in which a small number of coefficients contain a large proportion of the energy}.    
    \end{quote}
     
    Mathematically, a sparsity measure $S$ is a function with the mapping in Equation \ref{eq:sparsity}
     
    \begin{equation}
        \label{eq:sparsity}
        S: \bigg( \cup_{n \geq 1} \mathbb{C}^n \bigg) \rightarrow \mathbb{R}
    \end{equation} where $n \in \mathbb{N}$ is the number of coefficients. Although many functions preserve this mapping, it needs to follow several criteria to be considered a sparsity measure. The one that preserves most of these criteria and provides a way to measure the sparsity of a distribution is the Gini Index (GI) \abbrev{GI}{Gini Index} \cite{10.2307/2223525}, being originally proposed in economics to measure the inequality of wealth but was later demonstrated to be used as a sparsity measure \cite{2006SparseSources, 2004GiniIndexSpeech}. For a discrete signal, the GI is defined as follows.
    
    Given a vector $\textbf{v} = [v_1, v_2, \ldots, v_N]$ with its elements re-ordered and represented by $v_k$ where $|v_1| \leq |v_2| \ldots \leq |v_N|$, then the GI is defined as in Equation \ref{eq:gini_index}.
     
    \begin{equation}
        \label{eq:gini_index}
        GI(\textbf{v}) = 1 - 2\sum_{k=1}^{N} \frac{|v_k|}{||\textbf{v}||_1} \bigg( \frac{N-k+1/2}{N} \bigg)
    \end{equation}
     
    Besides being normalized for any vector, $GI(\textbf{v}) = 0$ represents the least sparse signal where all coefficients have an uniform amount of energy and $GI(\textbf{v}) = 1$ represents the most sparse signal with all the energy concentrated on a single coefficient \cite{10.1109/TIT.2009.2027527}. 
     
    In the context of RS, two types of sparsity measures can be explored: overall and specific sparsity measures \cite{10.1016/j.eswa.2010.09.141}.
    
    \subsubsection{Overall Sparsity} 

		The Overall Sparsity (OS) \abbrev{OS}{Overall Sparsity} measure of a $\mathbb{R}_{|\mathcal{U}| \times |\mathcal{I}|}$ utility matrix is defined as in Equation \ref{eq:os}
	
		\begin{equation}
			\label{eq:os}
			OS = 1 - \frac{|\mathcal{I}^+|}{|\mathcal{U}| \cdot |\mathcal{I}|}	 
		\end{equation} where $|\mathcal{I}^+|$ denotes the number of non-zero elements. 

	\subsubsection{User Specific Sparsity}
	
	In order to measure how sparse a given user $u$ is compared to all other users, the User Specific Sparsity (USS) \abbrev{USS}{User Specific Sparsity} measure for that single user is expressed as in Equation  \ref{eq:uss}

	\begin{equation}
		\label{eq:uss}
		USS = 1 - \frac{|\mathcal{I}_u|}{max_{u \in \mathcal{U}} (|\mathcal{I}_u|)}
	\end{equation} where $n_u$ is the number of evaluations input by user $u$. 
	
	\subsubsection{Item Specific Sparsity}
	
	Similarly, the Item Specific Sparsity (ISS) \abbrev{ISS}{Item Specific Sparsity} measure for each item $i$ is expressed in Equation \ref{eq:iss}
	
	\begin{equation}
	    \label{eq:iss}
	    ISS = 1 - \frac{|\mathcal{U}_i|}{max_{i \in \mathcal{I}} (|\mathcal{U}_i|)}
	\end{equation} where $n_i$ is the number of evaluations input to item $i$.
	
	Note that both USS and ISS are relative measures, in the sense that they compare how sparse a user and an item is to all other users and items, respectively.
	
