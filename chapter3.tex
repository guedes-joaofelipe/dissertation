This chapter focuses on describing the main technicalities of machine learning algorithms that are commonly used in RS and shall be tested in this research. Sparsity concepts are also presented in order to establish key definitions that support the proposed framework to evaluate sparsity effects on recommendation quality.

\section{Neighborhood Models}

As aforementioned in Section \ref{sec:CF}, neighborhood-based models use the principle of \textit{word-of-mouth} where the feedbacks of like-minded users are employed to compute item preferences on a cluster of peers. Since similarity between two entities are at the core of these models, two main approaches are described: the item- and user-based clustering.

User-based approach was applied in the early CF techniques, where users similarities were computed to estimate feedback prediction \cite{1999AlgorithmicFramework}. However, not only does user-user similarity matrices frequently present high dimensionality, but also does not provide helpful explanations on recommendations. To overcome these drawbacks, item-item similarity matrix is used instead, since the fact that usually $|\mathcal{U}| < |\mathcal{I}|$ results in a lower dimension similarity matrix \cite{2001sarwar}. With this perspective, the item-based clustering method is formally described as follows.

Given the set $\mathcal{N}_i(u)$ of $k$ most similar users to $u$ who have interacted with item $i$, $r_{ui}$ can be estimated according to Equation \ref{eq:itemknn}

\begin{equation} 
    \label{eq:itemknn}
    \hat{r}_{ui} = b_{ui} + \frac{\sum_{v \in \mathcal{N}_i(u)} \rho(u,v)\cdot (r_{vi}-b_{vi})}{\sum_{v \in \mathcal{N}_i(u)} \rho(u,v)}
\end{equation} where $\rho(u,v)$ is a distance or similarity measure between users $u$ and $v$. Several  measures have been used in this scenario, such as the Pearson correlation coefficient, the cosine similarity or the euclidean distance \cite{2010Handbook, 10.1145/3133264.3133299}.

%% Trocar similarity por distancia e usar outras distancias - Euclidean, Minkiwski, Mahalanobis, Cosine



\section{Latent Factor Models}

In latent factor approaches, it is assumed that data can be described by an underlying model. Formally, they can be abstracted as in Equation \ref{eq:latent_factor_models}

\begin{equation}
    \label{eq:latent_factor_models}
    \hat{r}_{ui} = \mathcal{F}(u,i|\Theta)
\end{equation} where $\mathcal{F}$ is an \textit{interaction function} that maps users and items latent factors $\mathbf{p}_u$ and $\mathbf{q}_i$, respectively, to the predicted feedback $\hat{r}_{ui}$, as $\Theta$ denotes a model's parameters to be obtained by a learning algorithm \cite{10.1145/3038912.3052569}.

The way these latent factors are combined vary from model to model, but two main approaches arise from recent literature: they can either be combined through dot product or learned similarity. Each of these scenarios are depicted in Figure \ref{fig:latent_factors}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figs/latent_factors.png}
    \caption{Users and items embeddings can be combined to estimate users' feedback through dot product (left) or learned similarities (right).}
    \label{fig:latent_factors}
\end{figure}

%Besides the model definition for learning $\Theta$, different approaches of objective function have been proposed in past literature, most of them fitting in one of two categories: point-wise and pair-wise objective functions [REF CF AUTOENCODERS].

Each of these approaches have their own model topologies and learning algorithms, which are presented as follows.

%Several approaches have been suggested to represent users and items embeddings and how they are combined to generate feedback predictions, the most commonly ones being presented as follows.  





\subsection{Matrix Factorization}

    Matrix factorization (MF) \abbrev{MF}{Matrix Factorization} models map users and items to a joint latent factor space of dimensionality $f$ such that user-item interactions are modeled as inner products in that space. Formally, the feedback $r_{ui}$ in this case is estimated as in Equation \ref{eq:MF}
    
    \begin{equation}
        \label{eq:MF}
        \hat{r}_{ui} = f(u,i|\mathbf{p}_u, \mathbf{q}_i) = \mathbf{p}_u^T \mathbf{q}_i = \sum_{k=1}^f p_{uk}q_{ik}
    \end{equation} where each item $i$ and user $u$ are mapped as vectors (latent factors) $\mathbf{q}_i, \mathbf{p}_u \in \mathbb{R}^f$ and $f$ is a hyper-parameter to be chosen \cite{10.1145/3038912.3052569,2009MFTechniques}.
    
    The goal of the learning algorithm is to estimate the parameters of $\mathbf{p}_u$ and $\mathbf{q}_i$. However, since most of the user-item interactions in $\mathbf{R}$ are unknown, the algorithm can only take into account the error associated with known entries of the matrix. Furthermore, in this set there are at least $(|\mathcal{U}|+|\mathcal{I}|)\cdot f$ parameters to be learned using at most $|\mathcal{U}|\cdot |\mathcal{I}|$ interactions, which is hardly the usual scenario since users most often interact with a small number of items. As a consequence, the learning algorithm needs to find a considerable number of parameters using few training samples, making over-fitting a problem to be dealt with \cite{2008ALSWR}.
    
   Several model definitions for learning $\Theta$, $\mathbf{p}_u$ and $\mathbf{q}_i$ have been proposed in past literature, each containing their own objective function. Hence, the most commonly used are defined as follows.
   
   \subsubsection{Alternating Least Squares}
    
    Let $\mathcal{K}$ be the set of $(u,i)$ pairs for which $r_{ui}$ is known. Then, the learning algorithm is designed to solve the following optimization problem: 
    
    \begin{equation}
        \label{eq:mf_min}
        \min_{\mathbf{q}^*, \mathbf{p}^*} \sum_{(u,i) \in \mathcal{K}} \bigg(r_{ui} - \mathbf{q}^T_i \cdot \mathbf{p}_u\bigg)^2 + \lambda \bigg(\sum_{i \in \mathcal{I}^+}||\mathbf{q}_i||^2 + \sum_{u \in \mathcal{U}^+}||\mathbf{p}_u||^2 \bigg)^2
    \end{equation} where $\lambda$ is a regularization parameter that can be estimated through cross-validation (probabilistic estimation is also proposed in \cite{2007ProbMF}). Two approaches are widely used to solve this optimization problem: the stochastic gradient descent and the alternating least squares. 
    
    By using Stochastic Gradient Descent (SGD) \abbrev{SGD}{Stochastic Gradient Descent} [Ref Simon Funk], the algorithm loops through all user-item pairs in a training set and computes an associated prediction error $e_{ui} \triangleq r_{ui} - \mathbf{q}^T_i \cdot \mathbf{p}_u$, updating the parameters by a magnitude proportional to a learning rate $\alpha$ as shown in Equation \ref{eq:sgd}.
    
    \begin{equation}
        \label{eq:sgd}
            \begin{cases}
            \mathbf{q}_i \leftarrow \mathbf{q}_i + \alpha (e_{ui}\cdot \mathbf{p}_u-\lambda \cdot \mathbf{q}_i) \\
            
            \mathbf{p}_u \leftarrow \mathbf{p}_u + \alpha (e_{ui}\cdot \mathbf{q}_i-\lambda \cdot \mathbf{p}_u)
            \end{cases}
    \end{equation} Although being simpler to implement, this approach is not suitable for parallelization since the learning algorithm needs to loop over all training set several times. This is a major drawback since most RS data sets have massive amount of data. In order to use the computational advantages of distributed systems, the Alternating Least Squares (ALS) \abbrev{ALS}{Alternating Least Squares} approach has been proposed by considering the minimization problem as follows.

    %PCA / SCA / ICA
    
    Due to the fact that both $\mathbf{q}_i$ and $\mathbf{p}_u$ are variables, Equation \ref{eq:mf_min} is not convex, leading to possible local minimum. On the other hand, if one of these variables are fixed, then the problem becomes quadratic and thus can be solved optimally. Consequently, the ALS switches between considering one variable fixed and solving the problem to the other, and then doing the opposite in a second step. The training steps can be summarized as follows \cite{2008ALSWR}:
    
    \begin{itemize}
        \item Step 1: Initialize matrix $\mathbf{P}$;
        \item Step 2: Fix $\mathbf{P}$ and solve $\mathbf{Q}$ by minimizing the objective function;
        \item Step 3: Fix $\mathbf{Q}$ and solve $\mathbf{P}$ by minimizing the objective function similarly;
        \item Step 4: Repeat Steps 2 and 3 until a stopping criterion is satisfied.
    \end{itemize}
    
    Additionally, weighted-$\lambda$-regularized (ALS-WR) is proposed in \cite{2008ALSWR} by modifying the regularization term as in Equation \ref{eq:mf_min_weighted}
    
    \begin{equation}
        \label{eq:mf_min_weighted}
        \min_{\mathbf{q}^*, \mathbf{p}^*} \sum_{(u,i) \in \mathcal{K}} (r_{ui} - \mathbf{q}^T_i \cdot \mathbf{p}_u)^2 + \lambda \Big(\sum_i n(i)||\mathbf{q}_i||^2 + \sum_u n(u)||\mathbf{p}_u||\Big)^2
    \end{equation} where $n(u)$ and $n(i)$ are the total number of interactions of user $u$ and item $i$, respectively.

\subsection{Multi-Layer Perceptron}

     Perceptrons are biological-inspired processing units that are composed by 3 basic elements: a sinapsis $\mathbf{w}$ that weights an entry $\mathbf{x}$, an aggregating function that adds a bias component $b$, and an activation function $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ \cite{Rosenblatt58theperceptron}. Together, they form an output signal $y$ that can be defined as in Equation \ref{eq:perceptron}.
     
     \begin{equation}
        y = \sigma (\mathbf{w}^T \mathbf{x} + b)
         \label{eq:perceptron}
     \end{equation}
     
    When these processing units are combined to form layers which are then stacked, a network as shown in Figure \ref{fig:mlp} is created and called a Multi-Layer Perceptron (MLP)\abbrev{MLP}{Multi-Layer Perceptron}. 
    
    \begin{figure}[ht]
        \centering
        \includegraphics[width=\textwidth]{figs/mlp.png}
        \caption{A MLP structure composed by an input, hidden and output layer.}
        \label{fig:mlp}
    \end{figure}
    
    Since multiple perceptrons are combined, one layer of a MLP can be defined as a function  $f: \mathbb{R}^{in} \rightarrow \mathbb{R}^{out}$ such that 
    
    \begin{equation*}
        f_{\mathbf{W}, \mathbf{b}}(\mathbf{x}) = \sigma(\mathbf{W}\mathbf{x} + \mathbf{b})
        \label{eq:layer}
    \end{equation*} which is parameterized by $\textbf{W} \in \mathbb{R}^{in \times out}$ and $\mathbf{b} \in \mathbb{R}^{out}$. Notice that $\sigma(\cdot)$ becomes an element-wise mapping function such that $\sigma(\mathbf{z}) = [\sigma(z_1), \sigma(z_2), \ldots, \sigma(z_{out})]$. The final definition of a MLP transfer function with $L$ layers is defined in Equation \ref{eq:mlp}.
    
    \begin{equation}
        f^{MLP}(\mathbf{x}) = f_{\mathbf{W_L}, \mathbf{b_L}}(\ldots f_{\mathbf{W_2}, \mathbf{b_2}}(f_{\mathbf{W_1}, \mathbf{b_1}} (\mathbf{x}))\ldots)
        \label{eq:mlp}
    \end{equation}
    
    It has been shown that such structure can serve as an universal function approximator if enough layers and/or processing units are provided \cite{10.5555/521706}. When applied to RS, they are used to learn interaction functions $\mathcal{F}$. Namely, one the most commonly used neural-based structures is the Neural Collaborative Filtering (NCF) \abbrev{NCF}{Neural Collaborative Filtering} \cite{10.1145/3038912.3052569}, which is defined as follows.
    
    Let $\mathbf{x} = [\mathbf{p}_u, \mathbf{q}_i]$ be the concatenation of users and items embeddings. In that case, Equation \ref{eq:latent_factor_models} results in
    
    \begin{equation*}
        \hat{r}_{ui} = \mathcal{F}(u,i|\Theta) = f_{\mathbf{W}_l, \mathbf{b}_l}(\ldots f_{\mathbf{W}_1, \mathbf{b}_1}([\mathbf{p}_u, \mathbf{q}_i]) \ldots)
    \end{equation*}

    \subsection{Auto-encoders}

    Given the flexibility of the previous MLP structure, one can take a non-linear approach of matrix factorization by setting a neural network to reconstruct its input signal, creating the so-called auto-encoder \cite{10.5555/2976456.2976476}. A classical auto-encoder, typically implemented with a single hidden layer, takes an input vector $\mathbf{x}$ and maps it to a hidden representation $\mathbf{z} \in \mathbb{R}^K$ such that
    
    \begin{equation*}
        \mathbf{z} = \sigma(\mathbf{W}\mathbf{x} + \mathbf{b})
    \end{equation*}which configures an encoder step since it usually maps the input vector to a lower dimension representation. A posterior decoding step can thus be implemented by setting the conventional MLP output $\mathbf{y}$ to be a reconstructed version of the input $\mathbf{x}$:
    
    \begin{equation*}
        \mathbf{y} = \hat{\mathbf{x}} = \sigma(\mathbf{W}'\mathbf{z} + \mathbf{b}')
    \end{equation*} where $\mathbf{W}'$ and $\mathbf{b}'$ are the weights associated with the decoder, which may be constrained with the encoder weights such that $\mathbf{W}' = \mathbf{W}$.
    
    Notice that a well-trained auto-encoder would simply learn the identity function $\mathbf{y} = \mathbf{x}$, which is hardly the goal of a RS since there is no point in reconstructing the input missing values. In that sense, a Denoising Auto-encoder extends its classical structure by training to reconstruct a corrupted version $\Tilde{\mathbf{x}}$ of the input, which can be obtained through additive Gaussian noise or multiplicative mask-out/drop-out noise \cite{10.1145/1390156.1390294}. 
    
    In addition, auto-encoders permit the addition of multiple input representations. For instance, the input signal $\mathbf{x}$ can contain both items and users representations in the input nodes. A final structure employed in RS that contains all the aforementioned characteristics is presented as a Collaborative Denoising Auto-encoder (CDAE) \abbrev{CDAE}{Collaborative Denoising Auto-encoder} \cite{10.1145/2835776.2835837}, represented in Figure \ref{fig:cdae}.
    
   \begin{figure}[ht]
        \centering
        \includegraphics[width=\textwidth]{figs/cdae.png}
        \caption{Structure of the Collaborative Denoising Auto-encoder.}
        \label{fig:cdae}
    \end{figure}

    In CDAE, the input layer is composed by $|\mathcal{I}|$ item nodes which receives the signals from the corrupted vector $\Tilde{\mathbf{r}}_u = [\Tilde{r}_{u1}, \Tilde{r}_{u2}, \ldots \Tilde{r}_{u|\mathcal{I}|}]$, and a user-specific node whose weights correspond to the elements of the embedding $\mathbf{p}_u$. Thus, the encoder is composed by a weight matrix $\mathbf{W} \in \mathbb{R}^{|\mathcal{I}| \times K}$ that connects the item input nodes to the hidden layer, and a user-specific $\mathbf{p}_u \in \mathbb{R}^K$ vector. 

    In addition to the $K$ nodes, the hidden layer contains a bias node. These hidden layer nodes are fully connected to the $|\mathcal{I}|$ nodes in the output layer through a weight matrix $\mathbf{W}' \in \mathbb{R}^{|\mathcal{I}| \times K}$ and a $\mathbf{b}' \in \mathbb{R}^{|\mathcal{I}|}$ vector. 
    
    Following the MLP equations, the CDAE initially maps the input to a latent representation $\mathbf{z}_u$ as:
    
    \begin{equation*}
        \mathbf{z}_u = \sigma(\mathbf{W}^T \Tilde{\mathbf{r}}_{u} + \mathbf{p}_u + \mathbf{b})
    \end{equation*} where $\sigma(\cdot)$ is usually the sigmoid function. Then, the output signal is computed as follows:
    
    \begin{equation*}
        \hat{\mathbf{r}}_{u} = \sigma(\mathbf{W}'^T\mathbf{z}_u + \mathbf{b}')
    \end{equation*}

    As aforementioned, the learning algorithm should not take into account the error in reconstructing the missing inputs. Therefore, a modified version of MLP learning algorithms based on reconstruction errors can be employed to learn the parameters, which is thoroughly detailed in \cite{10.1145/2835776.2835837}.

\section{Sparsity}

    Since sparsity is at the core of this research, its proper definition needs to be layed out. However, several sparsity concepts and measures have been proposed in the literature. This research focuses on the concept presented by Hurley et al. \cite{10.1109/TIT.2009.2027527}, which is defined as: 
    
    \begin{quote}
        \textit{A sparse representation is one in which a small number of coefficients contain a large proportion of the energy}.    
    \end{quote}
     
    Mathematically, a sparsity measure $S$ is a function with the mapping in Equation \ref{eq:sparsity}
     
    \begin{equation}
        \label{eq:sparsity}
        S: \bigg( \cup_{n \geq 1} \mathbb{C}^n \bigg) \rightarrow \mathbb{R}
    \end{equation} where $n \in \mathbb{N}$ is the number of coefficients. Although many functions preserve this mapping, it needs to follow several criteria to be considered a sparsity measure. The one that preserves most of these criteria and provides a way to measure the sparsity of a distribution is the Gini Index (GI) \abbrev{GI}{Gini Index} \cite{10.2307/2223525}, being originally proposed in economics to measure the inequality of wealth but was later demonstrated to be used as a sparsity measure \cite{2006SparseSources, 2004GiniIndexSpeech}. For a discrete signal, the GI is defined as follows.
    
    Given a vector $\textbf{v} = [v_1, v_2, \ldots, v_N]$ with its elements re-ordered and represented by $v_k$ where $|v_1| \leq |v_2| \ldots \leq |v_N|$, then the GI is defined as in Equation \ref{eq:gini_index}.
     
    \begin{equation}
        \label{eq:gini_index}
        GI(\textbf{v}) = 1 - 2\sum_{k=1}^{N} \frac{|v_k|}{||\textbf{v}||_1} \bigg( \frac{N-k+1/2}{N} \bigg)
    \end{equation}
     
    Besides being normalized for any vector, $GI(\textbf{v}) = 0$ represents the least sparse signal where all coefficients have an uniform amount of energy and $GI(\textbf{v}) = 1$ represents the most sparse signal with all the energy concentrated on a single coefficient \cite{10.1109/TIT.2009.2027527}. 
     
    In the context of RS, two types of sparsity measures can be explored: overall and specific sparsity measures \cite{10.1016/j.eswa.2010.09.141}.
    
    \subsubsection{Overall Sparsity} 

		The Overall Sparsity (OS) \abbrev{OS}{Overall Sparsity} measure of a $\mathbb{R}_{|\mathcal{U}| \times |\mathcal{I}|}$ utility matrix is defined as in Equation \ref{eq:os}
	
		\begin{equation}
			\label{eq:os}
			OS = 1 - \frac{|\mathcal{I}^+|}{|\mathcal{U}| \cdot |\mathcal{I}|}	 
		\end{equation} where $|\mathcal{I}^+|$ denotes the number of non-zero elements. 

	\subsubsection{User Specific Sparsity}
	
	In order to measure how sparse a given user $u$ is compared to all other users, the User Specific Sparsity (USS) \abbrev{USS}{User Specific Sparsity} measure for that single user is expressed as in Equation  \ref{eq:uss}

	\begin{equation}
		\label{eq:uss}
		USS = 1 - \frac{|\mathcal{I}_u|}{max_{u \in \mathcal{U}} (|\mathcal{I}_u|)}
	\end{equation} where $n_u$ is the number of evaluations input by user $u$. 
	
	\subsubsection{Item Specific Sparsity}
	
	Similarly, the Item Specific Sparsity (ISS) \abbrev{ISS}{Item Specific Sparsity} measure for each item $i$ is expressed in Equation \ref{eq:iss}
	
	\begin{equation}
	    \label{eq:iss}
	    ISS = 1 - \frac{|\mathcal{U}_i|}{max_{i \in \mathcal{I}} (|\mathcal{U}_i|)}
	\end{equation} where $n_i$ is the number of evaluations input to item $i$.
	
	Note that both USS and ISS are relative measures, in the sense that they compare how sparse a user and an item is to all other users and items, respectively.
	
