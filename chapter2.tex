In this chapter, the theoretical background on RS is presented. A brief history of its early applications is described, followed by the core objectives of the recommendation problem, its technical structures and classical topologies. Finally, different aspects of RS evaluations are presented.

\section{Brief History}

    The idea of using a computer-based filtering system according to users' preferences dates back to  the 1960s, when the early techniques were published by the name of ``Selective Dissemination of Information", where user-prompted keywords where employed to sort and filter documents \cite{1963SDI}. These techniques based on document representations created the foundations of what came to be known as content-based recommendations \cite{2016BeyondMatrixCompletion}. 

    Later in the 1980s \textit{Xerox PARC} implemented the \textit{Tapestry} system which was designed to filter emails according to user-specified rules. Such system even brought the idea of users     classifying other readers' messages, thus introducing the concept of collaborative filtering    \cite{2016BeyondMatrixCompletion}.

    By the 2000s the business impact of such systems started drawing attention after    \textit{Amazon.com} launched its online retailer store, whose revenue heavily relied on its recommendation engine. E-commerce companies then saw the strategic value of RS in such a way that, in 2007, \textit{Netflix} launched an open challenge rewarding 1 million dollars to the first team who managed to improve its recommendation engine in $10 \%$. More than 5000 teams registered for the competition and only in 2009 was the goal achieved by a group of researches, being a turning point in RS research \cite{2007TheNetflixPrize}.
    
    Despite bringing academic research forward, one of the consequences of the challenge was to steer the focus of research to the task of estimating users' preferences on unseen items or to provide users with ranked item lists. However, in practice RS can serve a variety of purposes other than these tasks and, therefore, a broader overview of the recommendation problem is needed in order to understand its true value.

  \section{The Recommendation Problem}

    As defined in \cite{2016BeyondMatrixCompletion}, the recommendation problem can be defined as: 

    \begin{quote}
      \textit{Find a sequence of conversational actions and item recommendations for each particular user that 
      optimizes the overall goal over the specified time-frame.}  
    \end{quote}
    
    In other words, this problem is formed by three components: an overall goal, a set of actions and     an optimization time-frame. 

    The \textbf{overall goal} represents a set of measures to be optimized by the RS. Traditionally, they are designed as accuracy or ranking metrics, such as precision and recall. However, measurable Key Performance Indicators (KPI) \abbrev{KPI}{key performance indicators} have also been used to define RS optimization goals that are more oriented towards business values, such as click-through rate, conversion, revenue and sales distribution  \cite{2019BusinessValue}.

    % As we can see, the business value of RS has grown over the years as more companies apply personalizations
    % as a strategy to improve their relationship with customers. Although is not always feasible to 
    % measure the true value of a RS, some business value measurements according to  
    % are presented in Figure \ref{fig:business_value}.

    % \begin{figure}
    %   \includegraphics[width=\linewidth]{./../img/rs_business_value.png}
    %   \caption{Overview of RS business value measurement approaches \cite{2019BusinessValue}.}
    %     \label{fig:business_value}
    % \end{figure}
    
    \textit{Click-through rate} (CTR)\abbrev{CTR}{Click-Through Rate}, for instance, is commonly used as a KPI in news recommendations as it measures the fraction of recommended items that were clicked. In \cite{2007GoogleNews} the authors show that personalized recommendations on Google's news personalization engine led to an increase in clicks by $38\%$ compared to a baseline that only recommends popular items. 

    % However, merely acessing a recommended item only grasps users' awareness towards products. In terms of appeal, 
    % \textit{Adoption and conversion} are applied measurements that 
    % indicates the depth to which users were interested in the clicked item. For instance, 
    % \textit{YouTube} uses the concept of ``long CTR'' where clicks on recommendations are only counted if the 
    % user subsequnetly watched a certain fraction of a video \cite{2010YoutubeVideoRS}. 

    % Furthermore, in web-based stores the impact of RS can be measured in terms \textit{sales and revenue}.
    % According to a statement of Amazon's CEO in 2006, about $35\%$ of their sales originated from recommendation \cite{2019BusinessValue}.
    % As we can see, the effectiveness of a RS can be measured in each step of a customers' journey on
    %  a marketing funnel as proposed by \cite{2016kotler}.

    Additionally, using item distribution aspects to optimization goals can bring revenue to niche products, highlighting one of the main competitive advantages of digital-based businesses according to the Long-Tail Theory \cite{2006LongTail}. For example, in \textit{Netflix} the ``Effective Catalog Size'' is a KPI that expresses the amount of catalog exploration by users \cite{2016NetflixBusinessValue}.

    % Finally, user engagement and behaviour can also be measured with viewability metrics as RS changes users' experience.
    % \textit{Netflix}, for example, disclosed that ``75\%'' of what people watch is from some sort of recommendation, later 
    % revealing that the recommendation engine business value is estimated on 1 billion US dollars per year \cite{2017Beyond5stars, 2010YoutubeVideoRS}.

    Besides presenting a list of recommended items, a \textbf{set of actions} concerning human-computer interface also needs consideration when designing a RS. It is known that the design to which items are presented plays a fundamental role in recommendation efficiency, so displaying suggestions in a proper frame is also part of the systems' design \cite{2012UserExperience}.
    
    Furthermore, the RS might also have better conversational features towards users other than merely receiving ratings and presenting items. Examples of such interactions are: wish-lists, explanations about recommendations and complementary proposals. This puts the user more into control and also   allows for new types of interactions \cite{2016BeyondMatrixCompletion}.  

    Finally, the \textbf{optimization time-frame} over which the goal should be optimized allows to differentiate between one-shot interactions and longer or even repeated time frames. Such component varies in different business application. For instance, in music streaming applications, recommendations might need to be repeated whereas in e-commerce or news portal scenarios the anonymous users require smaller time-frames to be considered. 

    Once these three components are set, an algorithm can be designed to meet these requirements. Most of them follows a data structure which is traditionally implemented as follows.

\section{Structure}
    For all the following sections, let $\mathcal{U}$ be the set of users and $\mathcal{I}$ be the set of items, where each user and item can be represented by their feature vectors $\mathbf{u}$ and $\mathbf{i}$, respectively. Also, consider $\mathcal{I}_u$ the set of items that user $u$ has interacted with, whereas  $\mathcal{U}_i$ is the set of users who interacted with item $i$. Since some users do not provide feedback to the system, $\mathcal{U}^+$ is the set of users who have at least one interaction - analogously, $\mathcal{I}^+$ is the set of items with interactions. 
    
    Therefore, the relationship between users and items can be represented by an utility-matrix $\mathbf{R} \in \mathbb{R}^{|\mathcal{U}| \times |\mathcal{I}|}$ as shown in Equation \ref{eq:MatrizUtilidade}
    
    \begin{equation}
        \mathbf{R} = 
        \kbordermatrix{ & i_1 & i_2 & \ldots & i_{|\mathcal{I}|} \\
        	u_1 & r_{11} & r_{12} & \ldots & r_{1|\mathcal{I}|} \\
        	u_2 & r_{21} & r_{22} & \ldots & r_{2|\mathcal{I}|}\\      
        	\vdots & \vdots & \vdots & \ddots & \vdots\\
        	u_{|\mathcal{U}|} & r_{|\mathcal{U}|1} & r_{\mathcal{U}2} & \ldots & r_{|\mathcal{U}||\mathcal{I}|}\\} \qquad
        \label{eq:MatrizUtilidade}      
    \end{equation} where the element $r_{ui} \in \mathbb{R}$ is measured by a feedback system, traditionally being one of the two types: explicit or implicit feedback.
    
    Explicit feedback are inputs that users intentionally provide through an interface, such as rating stars or thumbs-up/thumbs-down buttons. Since users do not usually rate all items in a catalog (or even the ones that they consume), this kind of feedback leads to a rather sparse utility matrix, affecting the quality of recommendation \cite{2010Handbook}. 
    
    On the other hand, implicit feedback is usually obtained by measuring users' behaviour in a digital-based platform, such as session time, clicks or page-views. Although leading to a less sparse utility matrix, this type of feedback might generate noisy data as these measurements do not differentiate whether the user is actually consuming the item or not. For instance, when a user access a news page he does not necessarily reads it \cite{2008ImplicitFeedback}.
    
    A hybrid feedback might also be implemented as shown in \cite{2015RSPrinciples}. Additionally, some implementations of the utility matrix store other user- or item-related data, such as users' meta-data. These particularities shall be described for each recommendation algorithm in Chapter \ref{chap:3}. 
    
    Once a data model is structured, one of the commonly used goals of the RS is to predict the missing entries from the matrix. To that end, two filtering topologies are traditionally used: Content-based Filtering and Collaborative Filtering. 

\subsection{Content-Based Filtering}

    Content-based Filtering (CB) \abbrev{CB}{Content-based Filtering} is a family of methods that, at their core, seeks to match users and items by their features. The focus is to use these features to raise users' consumption profiles, creating vector representations of the types of items they are leaned to consume and computing items similar to that consumption vector \cite{2016Textbook}.
    
    In this case, only the target-user's interactions with the system are necessary, disregarding other users' behaviours. The positive impact of such architecture is that less information about the user is needed to infer his preferences. For instance, if a child searches for a movie to watch on a streaming platform, kids-oriented items can be suggested based on demographic features. It has also been proven that this is an effective way to reduce the \textit{item cold-start}, since the item's feature might be sufficient for it to start being recommended \cite{10.1145/564376.564421}. Also, by using the features of items to build a recommendation list, it provides transparency and explanations on why that item is being suggested, which has also been proven to be a contributing factor to build users' trust in the system \cite{10.1145/1111449.1111475}.

    On the opposite direction, a major drawback of such topology is that CB tends to recommend the same types of items - commonly known as the \textit{bubble effect} - and thus it suffers from overspecialization, which has been tackled by increasing serendipity in the RS \cite{10.1109/HIS.2008.25}. Furthermore, some applications may not easily provide items' or users' features and therefore poor representations are extracted. Examples of such limitations occur on video, audio or image recommendation, such as the one used at the \textit{Music Genome Project} employed by the online radio service \textit{Pandora.com}  \cite{2009MFTechniques}.

\subsection{Collaborative Filtering}
\label{sec:CF}

    Collaborative Filtering (CF) \abbrev{CF}{Collaborative Filtering} is another popular family of techniques which underlies in the assumption     that users with similar relevance on past items will probably agree on evaluating future items the same way. In that sense, users' relevance to items can be estimated by their peers' feedback.

    Given these characteristics of CF, two groups of methods are usually portrayed in the literature: the memory-based CF and the model-based CF \cite{2010Handbook}. 

  \subsubsection{Memory-based method}

    The memory-based method creates recommendations based on the feedback of points in a cluster - which is why it is also called the neighborhood method. Two approaches guide the creation of such groups: the user- and the item-based CF.
    
    %uses the utility matrix to calculate the similarity between users or items and create clusters that are used to estimate preferences. based on past feedback. In that sense, two approaches can be made: the user-based CF and the item-based CF \cite{2016Textbook, 2011ekstrand}. 
    
    Being firstly proposed in the \textit{GroupLens} Usnet article recommender, the user-based CF tries to find peers of users with similar past consumption behaviour \cite{10.1145/192844.192905}. By finding a target-user's peers, their feedback for an item is weighted by their level of similarity with the target-user.
    
    Despite effective, user-based CF suffers from scalability issues when the user-item ratio grows, which typically happens in e-commerce environments where operations on large similarity matrices are required \cite{10.1016/j.ins.2007.07.024, 10.5555/2842045.2842365, 10.1016/j.knosys.2015.03.001}. To overcome this issue, item-based CF uses similarities between the evaluation patterns of items, yielding a lower dimensional and more stable similarity matrix than its user-based counterpart \cite{2003amazon, 10.1155/2009/421425}. In its overall structure, it is similar to content-based recommendation, but instead of relying primarily on item's metadata, it deduces items' similarities from user preference patterns \cite{2001sarwar}.

    A critical design decision in implementing this approach is the choice of similarity functions to compute the neighborhood. Traditional functions are the Pearson correlation coefficient and the cosine similarity measure. However, these traditional functions have drawbacks such as providing poor similarity representations when a small number of co-rated items are considered, which frequently happen in sparse matrices \cite{2016Textbook}. Enhancements proposals for these functions have been made in past research and an overview of them can be found in \cite{10.1145/3133264.3133299}.

    On the other hand, several advantages can be listed for the memory-based CF. Besides the ease of implementation of similarity functions, memory-based CF are online learning models, providing great advantages to the arrival of new data and, therefore, to the adaptation of concept drifts to users' preferences. Finally, like the CB topology, explanations about recommendations can be provided to users, which has shown to increase users' trust in the system as aforementioned.

    An example of item-based CF was shown in \cite{2003amazon}, where authors use a recommendation algorithm to personalize the online store for each customer at \textit{Amazon.com}, demonstrating the scalability advantage of this approach \cite{2003amazon}.

  \subsubsection{Model-based CF}

  Model-based CF rely on using past feedback values to train a model that learn an interaction function between users and items. Besides using feedback data, these types of techniques have been outperformed many other approaches by also using item's or user's metadata, which are useful tools for reducing cold-start issues.

    To learn such interaction functions, most model-based techniques map both users and items to their respective latent factors in such a way that they can be compared in this latent factor space. To that end, matrix factorization is a classic approach which can be performed in many ways, such as in singular value decomposition or through a neural-based auto-encoder approach. Some of these structures and their learning algorithms shall be thoroughly described in Chapter \ref{chap:3}.

    Finally, both CF and CB approaches can be mixed to form a hybrid recommendation algorithm that captures the best characteristics of the underlying structures. For instance, one can make an ensemble recommendation engine by weighting the output of different algorithms or by cascading the output of one into the input of another. Although robust, a hybrid RS tends to create a complex implementation as a major drawback, making it hard to be used in practical scenarios. A thorough explanation of hybrid recommendation methods are described in \cite{10.1023/A:1021240730564}.
%\subsection{Hybrid Topologies}


\section{Evaluation}
\label{sec:evaluation}

%Rever artigo "Beyond Matrix Completion" para mais informações

%https://sci-hub.se/https://dl.acm.org/doi/10.1145/3383313.3412489

Designing a RS requires understanding its different properties and selecting which one best applies to the business' objectives. However, some properties may naturally conflict with one another. For instance, improvements on recommendation accuracy may come at a cost of having lower diversity. 

In order to analyze the trade-offs between a system's properties, one should naturally implement a way to measure them. In that sense, several classes of properties rose in the literature, like prediction accuracy and coverage. Hence, they are formally described in the following sections. 

\subsection{Prediction Accuracy}

At the core of most RS lies a prediction engine and, traditionally, a basic assumption is that a system which provides more accurate predictions will be preferred by users - although a drawback of these assumptions has become a matter of discussion in recent research as discussed in the following chapter. 

Nevertheless, due to the broad usage of prediction accuracy as a way to evaluate recommendation quality, the following classes of measures are defined: rating prediction accuracy, usage prediction accuracy and ranking prediction accuracy.

\subsubsection{Rating Prediction Accuracy}

In applications where explicit feedback is provided, a property to be measure in a RS is its capacity to estimate accurately the user's rating. One of the most popular metrics used for such objective is the Root Mean Squared Error (RMSE) \abbrev{RMSE}{Root Mean Squared Error} defined in Equation \ref{eq:rmse}

\begin{equation}
\label{eq:rmse}
    RMSE = \sqrt{\frac{1}{|\mathcal{U}| \cdot |\mathcal{I}|}\sum_{(u,i) \in \mathcal{K}}(r_{ui} - \hat{r}_{ui})^2}
\end{equation} where $\mathcal{R}$ is the set of all known user-item interaction pairs.

\subsubsection{Usage Prediction Accuracy}

Usage prediction measures are employed on systems that are designed to predict which items consumers are most probably to use rather than predicting their ratings. In this set, after a RS generates a list of $N$ recommendations, four possible outcomes are presented on Table \ref{tab:usage_table}.

\begin{table}[h]
	\centering
	\begin{tabular}{@{}ccc@{}}
		& \textbf{Recommended}                     & \textbf{Not recommended}                 \\ 
		\multicolumn{1}{c|}{\textbf{Consumed}}     & \multicolumn{1}{c|}{True-positive (TP)}  & \multicolumn{1}{c|}{False-negative (FN)} \\ 
		\multicolumn{1}{c|}{\textbf{Not consumed}} & \multicolumn{1}{c|}{False-positive (FP)} & \multicolumn{1}{c|}{True-negative (TN)}  \\ 
	\end{tabular}
	\caption{Possible outcomes of usage prediction-oriented recommendations.}
	\label{tab:usage_table}
\end{table}

By making the same operation to all users and counting the occurrences on each outcome, classification metrics $precision@K$ and $recall@K$ can be defined as Equations \ref{eq:precision} and \ref{eq:recall}, respectively.

\begin{equation}
\label{eq:precision}
Precision@K = \frac{\#TP}{\#TP + \#FP}
\end{equation}

\begin{equation}
\label{eq:recall}
Recall@K = \frac{\#TP}{\#TP + \#FN}
\end{equation}

\subsubsection{Ranking Prediction Accuracy}

Usually applied in the evaluation of search query engines, ranking measures are also employed to acess the quality of a RS when the order of items is essential to system, being the case where users have to browse through tracks or pages of items, for example. In these applications, the aim is to predict not necessarily the explicit feedback but rather the order of the items according to users' preferences. 

Let $\mathcal{R}$ be a ranked list of recommendations. A direct way of computing the effectiveness of ranking is through the Mean Reciprocal Rank (MRR) \abbrev{MRR}{Mean Reciprocal Rank} as defined in Equation \ref{eq:mrr}

\begin{equation}
\label{eq:mrr}
MRR = \frac{1}{|\mathcal{R}|}\sum_{i \in \mathcal{R}} \frac{1}{rank_i}
\end{equation} where $rank_i \geq 1$ is an integer indicating the rank of a consumed item $i$. 

Notice that the MRR only takes into account the rank of an item, disregarding its relevance to the user. With the intent to penalize not only wrongly-ordered item but also unsuited ones, the Discounted Cumulative Gain (DCG) \abbrev{DCG}{Discounted Cumulative Gain} \cite{10.1145/582415.582418} represents a weighted sum of relevance's degrees for each ranked items. Its most common definition is shown in Equation \ref{eq:dcg}

\begin{equation}
    \label{eq:dcg}
    DCG = \sum_{i \in \mathcal{R}} \frac{1}{\log_2(rank_i+1)} \cdot g(r_{ui})
\end{equation} where $g(r_{ui})$ is a gain function of the relevance - which is traditionally the identity function, although alternative forms are also employed. For example, placing stronger emphasis on more relevant items can be achieved by defining $g(r_{ui}) = 2^{r_{ui}}-1$. Notice that the logarithmic part of the equation can also be changed to define other discounting approaches towards the rank, which can be found more formally in \cite{NDCGref}.

An ideal ordered list can be created by sorting all relevant items by their relative relevance, producing the maximum value for DCG - yielding the \textit{Ideal} DCG ($IDCG$) \abbrev{IDCG}{Ideal Discounted Cumulative Gain}. This allows to normalize the obtained DCG and create the so-called Normalized Discounted Cumulative Gain (NDCG) \abbrev{NDCG}{Normalized Discounted Cumulative Gain}, as defined by Equation \ref{eq:NDCG}.

\begin{equation}
	\label{eq:NDCG}
	NDCG = \frac{DCG}{IDCG}
\end{equation}

\subsection{Coverage}

The previous property of prediction accuracy is tightly related to the amount of data the system has about users and items - ideally, more data yields better accuracy. Therefore, some algorithms may provide high quality recommendations only for a small fraction of users of items. Since this quality is usually used as threshold for item filtering, item or user space coverage are possible evaluations to be analyzed on a RS. 

Item Space Coverage (ISC) \abbrev{ISC}{Item Space Coverage} - also referred to as catalog coverage - is the most commonly used approach and it is defined in its simplest form as in Equation \ref{eq:item_space_coverage}

\begin{equation}
    \label{eq:item_space_coverage}
    ISC = \frac{|\cup_{u \in \mathcal{U}} \mathcal{R}_u|}{|\mathcal{I}|}
\end{equation} which qualitatively is the percentage of items for which a recommendation system can provide predictions \cite{1999AlgorithmicFramework}. However, in some cases users are also kept from receiving recommendations due to various business rules, such as insufficient amount of data or anonymity, leading to the analogous definition of User Space Coverage (USC) \abbrev{USC}{User Space Coverage} \cite{2011EvaluatingRS}.

%falar sobre cold start


%\subsection{Confidence}

%\subsection{Trust}

\subsection{Diversity}

%However, much like the novelty property of a RS, high diversity also comes at the expense of good accuracy \cite{2011NoveltyDiversityTopN}.

Displaying a broader array of choices not only helps business bring revenue to long tail market niches, but mainly help users discover new interests - which is often the case that they rely on a RS. Therefore, evaluating diversity can be a good strategy to optimize the chances that at least some item pleases the user.

Formally, one of the most frequently considered metric to evaluate this property is the so-called Intra-List Diversity (ILD) \abbrev{ILD}{Intra-List Diversity} \cite{2001SimilarityDiversity} which is defined as in Equation \ref{eq:diversity}

\begin{equation}
    \label{eq:diversity}
    ILD (\mathcal{R}_u) = \frac{1}{|\mathcal{R}_u||\mathcal{R}_u-1|} \sum_{i, j \in \mathcal{R}_u} dist(i,j)
\end{equation} where $dist(i,j)$ is a configurable distance measure between items. Such distance is generally a function of item features \cite{10.1145/1060745.1060754} or the distance in terms of interaction patterns by users \cite{2014NoveltyDiversityEnhancement}.


\subsection{Novelty}

Exposing users to a relevant experience that they would not have found by themselves is a desirable feature in RS, since obvious yet accurate recommendations do not suffice. To assess the level of such exposure, novelty is generally considered, being a piece of information that refers to how different it is with respect to what users have seen in the past \cite{2011RankNoveltyDiversity}. 

Many definitions of novelty have been proposed in the literature, but as a time component is at the core of this property, Temporal Novelty (TN) \abbrev{TR}{Temporal Novelty} is usually a common approach that is defined as in Equation \ref{eq:novelty}

\begin{equation}
\label{eq:novelty}
    TN(\mathcal{R}^t_u) = \frac{|\mathcal{R}^t_u \setminus \cup_{\tau < t} \mathcal{R}^t_u|}{|\mathcal{R}^t_u|}
\end{equation} where $\mathcal{R}^t_u$ is the set of recommended items for user $u$ at time $t$. Having such definition may serve as a guideline for improving users' retention as studies show that their perception towards recommendation lists degrades over they if they do not receive novelty over past recommendations. This becomes more latent with users who have fewer interactions, who tend to receive less novel recommendations.

Although being generally understood as the difference between present and past experience, other types of novelty perspectives can be considered. Long tail novelty, for instance, considers how novel are items based on their popularity, which is specially used to evaluate whether RS are biased towards obvious recommendations of very popular items \cite{2014NoveltyDiversityEnhancement}.

\subsection{Serendipity}

Notice that in the definitions of novelty so far, no restriction on item relevance was made. To combine both unexpectedness with a positive response, the notion of serendipity is applied as particularization of novelty. Formally, it can be defined as in Equation 

\begin{equation}
    \label{eq:serendipity}
    Srdp(\mathcal{R}_u) = \frac{|(\mathcal{R}_u \setminus PM) \cap Rel_u|}{|\mathcal{R}_u|}
\end{equation} where $PM$ is a set of items predicted by a primitive method such as already seen items or most popular items and $Rel_u$ is a set of items that the user $u$ finds relevant \cite{2014NoveltyDiversityEnhancement}.

%\section{Experimentation}


