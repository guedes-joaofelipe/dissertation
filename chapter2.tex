In this chapter, the theoretical background on RS is presented. A brief history of its early applications is depicted, followed by the core objective of the recommendation problem, its technical structures and classical topologies. Finally, different aspects of RS evaluations is presented.

\section{Brief History}

    The idea of using a computer-based filtering system according to users' preferences dates back to  the 1960s, when the early techniques were published by the name of ``Selective Dissemination of Information", where user-prompted keywords where employed to sort and filter documents \cite{1963SDI}. These techniques based on document representations created the foundations of what came to be known as content-based recommendations \cite{2016BeyondMatrixCompletion}. 

    Later in the 1980s \textit{Xerox PARC} implemented the \textit{Tapestry} system which was designed to filter emails according to user-specified rules. Such system even brought the idea of users     classifying other readers' messages, thus introducing the concept of collaborative filtering    \cite{2016BeyondMatrixCompletion}.

    By the 2000s the business impact of such systems started drawing attention after    \textit{Amazon.com} launched its online retailer store, whose revenue heavily relied on its recommendation engine. E-commerce companies then saw the strategic value of RS in such a way that, in 2007, \textit{Netflix} launched an open challenge rewarding 1 million dollars to the first team who managed to improve its recommendation engine in $10 \%$. More than 5000 teams registered for the competition and only in 2009 was the goal achieved by a group of researches, being a turning point in RS research \cite{2007TheNetflixPrize}.
    
    Despite bringing academic research forward, one of the consequences of the challenge was to steer  the focus of research to the task of estimating users' preferences of unseen items or to provide users with ranked item lists. However, in practice RS can serve a variety of purposes other than these tasks and, therefore, a broader overview of the recommendation problem is needed in order to understand its true value.

  \section{The Recommendation Problem}

    As defined in \cite{2016BeyondMatrixCompletion}, the recommendation problem can be defined as: 

    \begin{quote}
      \textit{Find a sequence of conversational actions and item recommendations for each particular user that 
      optimizes the overall goal over the specified time-frame.}  
    \end{quote}
    
    In other words, this problem is formed by three components: an overall goal, a set of actions and     an optimization time-frame. 

    The \textbf{overall goal} represents a set of measures to be optimized by the RS. Traditionally, they are designed as accuracy or ranking metrics, such as precision and recall. However, measurable key performance indicators (KPI) \abbrev{KPI}{key performance indicators} have also been used to define RS optimization goals that are more oriented towards business values, such as click-through rate, conversion, revenue and sales distribution  \cite{2019BusinessValue}.

    % As we can see, the business value of RS has grown over the years as more companies apply personalizations
    % as a strategy to improve their relationship with customers. Although is not always feasible to 
    % measure the true value of a RS, some business value measurements according to  
    % are presented in Figure \ref{fig:business_value}.

    % \begin{figure}
    %   \includegraphics[width=\linewidth]{./../img/rs_business_value.png}
    %   \caption{Overview of RS business value measurement approaches \cite{2019BusinessValue}.}
    %     \label{fig:business_value}
    % \end{figure}
    
    \textit{Click-through rate} (CTR) \abbrev{CTR}{click-through rate}, for instance, is commonly used as a KPI in news recommendations as it measures the fraction of recommended items that were clicked. In \cite{2007GoogleNews} the authors show that personalized recommendations on Google's news personalization engine led to an increase in clicks by $38\%$ compared to a baseline that only recommends popular items. 

    % However, merely acessing a recommended item only grasps users' awareness towards products. In terms of appeal, 
    % \textit{Adoption and conversion} are applied measurements that 
    % indicates the depth to which users were interested in the clicked item. For instance, 
    % \textit{YouTube} uses the concept of ``long CTR'' where clicks on recommendations are only counted if the 
    % user subsequnetly watched a certain fraction of a video \cite{2010YoutubeVideoRS}. 

    % Furthermore, in web-based stores the impact of RS can be measured in terms \textit{sales and revenue}.
    % According to a statement of Amazon's CEO in 2006, about $35\%$ of their sales originated from recommendation \cite{2019BusinessValue}.
    % As we can see, the effectiveness of a RS can be measured in each step of a customers' journey on
    %  a marketing funnel as proposed by \cite{2016kotler}.

    Additionally, using item distribution aspects to optimization goals can bring revenue to niche products, highlighting one of the main competitive advantages of digital-based businesses according to the Long-Tail Theory \cite{2006LongTail}. For example, in \textit{Netflix} the ``Effective Catalog Size'' is a KPI that expresses the amount of catalog exploration by users \cite{2016NetflixBusinessValue}.

    % Finally, user engagement and behaviour can also be measured with viewability metrics as RS changes users' experience.
    % \textit{Netflix}, for example, disclosed that ``75\%'' of what people watch is from some sort of recommendation, later 
    % revealing that the recommendation engine business value is estimated on 1 billion US dollars per year \cite{2017Beyond5stars, 2010YoutubeVideoRS}.

    Besides presenting a list of recommended items, a \textbf{set of actions} concerning human-computer interface also needs consideration when designing a RS. It is known that the design to which items as presented plays a fundamental role in recommendation efficiency \cite{2012UserExperience}, so displaying suggestions in a proper frame is also part of the systems' design. 
    
    Furthermore, the RS might also have better conversational features towards users other than merely receiving ratings and presenting items. Examples of such interactions are: wish-lists, explanations about recommendations and complementary proposals. This puts the user more into control and also   allows for new types of interactions \cite{2016BeyondMatrixCompletion}.  

    Finally, the \textbf{optimization time-frame} over which the goal should be optimized allows to differentiate between one-shot interactions and longer or event repeated time frames. Such component varies in different business application. For instance, in music streaming applications, recommendations might need to be repeated whereas in e-commerce or news portal scenarios the anonymous users require smaller time-frames be considered. 

    Once these three components are set, an algorithm can be designed to meet these requirements. Most of them follows a data structure which is traditionally implemented as follows.

\section{Mathematical Definitions}
    For all the following sections, let $\mathcal{U}$ be the set of users with size $|\mathcal{U}|$ and $\mathcal{I}$ be the set of items with size $|\mathcal{I}|$, where each user and item can be represented by their features vectors $\mathbf{u}$ and $\mathbf{i}$, respectively. Consequently, consider that $\mathbf{R}$ denotes an utility matrix where each element $r_{ui}$ represents a feedback between user $u$ and item $i$. 

\section{Structure}

    In a personalized recommendation, the relationship between users and items is commonly represented in a utility-matrix $\mathbf{R}$ as shown in Equation \ref{eq:MatrizUtilidade}
    
    \begin{equation}
        \mathbf{R}_{|\mathcal{U}| \times |\mathcal{I}|} = 
        \kbordermatrix{ & i_1 & i_2 & \ldots & i_{|\mathcal{I}|} \\
        	u_1 & r_{11} & r_{12} & \ldots & r_{1|\mathcal{I}|} \\
        	u_2 & r_{21} & r_{22} & \ldots & r_{2|\mathcal{I}|}\\      
        	\vdots & \vdots & \vdots & \ddots & \vdots\\
        	u_{|\mathcal{U}|} & r_{|\mathcal{U}|1} & r_{\mathcal{U}2} & \ldots & r_{|\mathcal{U}||\mathcal{I}|}\\} \qquad
        \label{eq:MatrizUtilidade}      
    \end{equation} where the element $r_{ij}$ is usually measured by a feedback system. In this case, two types of feedback can be seen in traditional applications: explicit and implicit feedbacks.
    
    Explicit feedback are inputs that users intentionally give through an interface, such as rating stars or thumbs-up/thumbs-down buttons. Given that users do not usually rate all items in a catalog (or even the ones that they consumed), this kind of feedback leads to a rather sparse utility matrix, affecting the quality of recommendation \cite{2010Handbook}. 
    
    On the other hand, implicit feedback is usually obtained by measuring users' behaviour in a digital-based platform, such as session time, clicks or page-views. Although leading to a less sparse utility matrix, this type of feedback might generate noisy data as these measurements do not differentiate whether the user is actually consuming the item or not. For instance, when a user access a news page he does not necessarily reads it \cite{2008ImplicitFeedback}.
    
    A hybrid feedback might also be implemented as shown in \cite{2015RSPrinciples}. Additionally, some implementations of the utility matrix store other user- or item-related data, such as users' meta-data. These particularities shall be described for each algorithm in the next chapter. 
    
    Once a data model is structured, one of the commonly used goals of the RS is to predict the missing entries from the matrix. To that end, three filtering topologies are traditionally used: content-based filtering, collaborative filtering and hybrid filtering. 

\subsection{Content-Based Filtering}

Content-based Filtering (CB) \abbrev{CB}{Content-based Filtering}

\subsection{Collaborative Filtering}
\label{sec:CF}

    Collaborative Filtering (CF) \abbrev{CF}{Collaborative Filtering} is a popular family of techniques which underlies in the assumption 
    that users with similar relevance on past items will probably agree on evaluating future items 
    the same way [REF, melhorar]. In that sense, users' relevance to items can be estimated by their
    peers' feedback.

    Given these characteristics of CF, two groups of methods are usually portrayed in the literature: 
    the memory-based CF and the model-based CF [REF, Survey on Recommendation Systems 2016 Gaudani]. 

  \subsubsection{Memory-based CF}

    In the memory-based method, the utility matrix is used to calculate the similarity between 
    users or items based on past ratings or interactions. In that sense, two approaches can be made:
    the \textit{user-user} CF and the \textit{item-item} CF \cite{2011ekstrand}. 
    
    The user-user CF, also known as \textit{k-nearest neighbors} CF tries to find users' peers 
    whose past evaluation behaviour is similar to the target user. By finding these peers, their 
    feedback for an item in question is weighted by their level of agreement with the target user.  
    
    A critical design decision in implementing this approach is the choice o similarity functions. 
    Several functions have been proposed in the literature, such as the Pearson correlation, the 
    Spearman rank correlation or the Cosine similarity. An overview of these functions applied to RS
    can be found in \cite{2011ekstrand}.
    
    While effective, user-user CF suffers from scalability problems when the user-item ratio grows, which typically happens in e-commerce environments (thus requiring operations on larger similarity matrices). To overcome this issue, \textit{item-item} CF uses similarities between the evaluation patterns of items. In its overall structure, it is similar to content-based recommendation, but instead of relying primarily on item's metadata, it deduces items' similarities from user preference patterns \cite{2001sarwar}.

    Several advantages can be listed for the memory-based CF. Given that mostly rely on similarity     functions, their algorithms can be easily implemented. Furthermore, memory-based CF are online     learning models, which provides great advantages to the arrival of new data and, therefore, to the adaptation of concept drifts to users' preferences [REF]. Finally, explanations about the     recommendations can be provided to users, which has shown to increase users' trust in the system     [REF, ZHENG 2016].

    However, data sparsity poses a great disadvantage to these methods. Since the utility matrix is 
    sparse in most applications, the efficiency in similarity calculations is compromised by the lack 
    of sufficient data [REF, AGARRWAL, Recommender SYstems: The textbook 2016]
    
    An example of item-item CF was shown in \cite{2003amazon}, where authors use a recommendation 
    algorithm to personalize the online store for each customer at \textit{Amazon.com}, demonstrating 
    the scalability advantage of this approach.

  \subsubsection{Model-based CF}

  Model-based CF rely on data mining and machine learning methods to find underlying patterns on 
  users' preferences based on their feedback (although users' and items' metadata can be also used to 
  improve recommendation performance [REF]). 

  [Vantagens]

  [Desvantagens]

  [Exemplos]




\section{Evaluation}

%Rever artigo "Beyond Matrix Completion" para mais informações

%https://sci-hub.se/https://dl.acm.org/doi/10.1145/3383313.3412489

Designing a RS requires understanding its different properties and selecting which one best applies to the business' objectives. However, some properties may naturally conflict with one another. For instance, improvements on recommendation accuracy may come at a cost of having lower diversity. 

In order to analyze the trade-offs between a system's properties, one should naturally implement a way to measure them. In that sense, several classes of properties rose in the literature, like prediction accuracy and coverage. Hence, they are formally described in the following sections. 

\subsection{Prediction Accuracy}

At the core of most RS lies a prediction engine and, traditionally, a basic assumption is that a system which provides more accurate predictions will be preferred by users - although a drawback of these assumptions has become a matter of discussion in recent research as discussed in the following chapter. 

Nevertheless, due to the broad usage of prediction accuracy as a way to evaluate recommendation quality, the following classes of measures are defined: rating prediction accuracy, usage prediction accuracy and ranking prediction accuracy.

\subsubsection{Rating Prediction Accuracy}

In applications where explicit feedback is provided, a property to be measure in a RS is its capacity to estimate accurately the user's rating. One of the most popular metrics used for such objective is the Root Mean Squared Error (RMSE) \abbrev{RMSE}{Root Mean Squared Error} defined in Equation \ref{eq:rmse}.

\begin{equation}
\label{eq:rmse}
    RMSE = \sqrt{\frac{1}{|\mathcal{U}| \cdot |\mathcal{I}|}\sum_{(u,i) \in \mathcal{K}}(r_{ui} - \hat{r}_{ui})^2}
\end{equation}

\subsubsection{Usage Prediction Accuracy}

Usage prediction measures are employed on systems that are designed to predict which items consumers are most probably to use rather than predicting their ratings. In this set, after a RS generates a list of $N$ recommendations, four possible outcomes are presented on Table \ref{tab:usage_table}.

\begin{table}[h]
	\centering
	\begin{tabular}{@{}ccc@{}}
		& \textbf{Recommended}                     & \textbf{Not recommended}                 \\ 
		\multicolumn{1}{c|}{\textbf{Consumed}}     & \multicolumn{1}{c|}{True-positive (TP)}  & \multicolumn{1}{c|}{False-negative (FN)} \\ 
		\multicolumn{1}{c|}{\textbf{Not consumed}} & \multicolumn{1}{c|}{False-positive (FP)} & \multicolumn{1}{c|}{True-negative (TN)}  \\ 
	\end{tabular}
	\caption{Possible outcomes of usage prediction-oriented recommendations.}
	\label{tab:usage_table}
\end{table}

By making the same operation to all users and counting the occurrences on each outcome, classification metrics $precision@K$ and $recall@K$ can be defined as Equations \ref{eq:precision} and \ref{eq:recall}, respectively.

\begin{equation}
\label{eq:precision}
Precision@K = \frac{\#TP}{\#TP + \#FP}
\end{equation}

\begin{equation}
\label{eq:recall}
Recall@K = \frac{\#TP}{\#TP + \#FN}
\end{equation}

\subsubsection{Ranking Prediction Accuracy}


\subsection{Coverage}

The previous property of prediction accuracy is tightly related to the amount of data the system has about users and items - ideally, more data yields better accuracy. Therefore, some algorithms may provide high quality recommendations only for a small fraction of users of items. Since this quality is usually used as threshold for item filtering, item or user space coverage are possible evaluations to be analyzed on a RS. 

Item Space Coverage (ISC) \abbrev{ISC}{Item Space Coverage} - also referred to as catalog coverage - is the most commonly used approach and it is defined in its simplest form as in Equation \ref{eq:item_space_coverage}

\begin{equation}
    \label{eq:item_space_coverage}
    ISC = \frac{|\cup_{u \in \mathcal{U}} \mathcal{R}_u|}{|\mathcal{I}|}
\end{equation} which qualitatively is the percentage of items for which a recommendation system can provide predictions \cite{1999AlgorithmicFramework}. However, in some cases users are also kept from receiving recommendations due to various business rules, such as insufficient amount of data or anonymity, leading to the analogous definition of User Space Coverage (USC) \abbrev{USC}{User Space Coverage} \cite{2011EvaluatingRS}.

%falar sobre cold start


%\subsection{Confidence}

%\subsection{Trust}

\subsection{Diversity}

%However, much like the novelty property of a RS, high diversity also comes at the expense of good accuracy \cite{2011NoveltyDiversityTopN}.

Displaying a broader array of choices not only helps business bring revenue to long tail market niches, but mainly help users discover new interests - which is often the case that they rely on a RS. Therefore, evaluating diversity can be a good strategy to optimize the chances that at least some item pleases the user.

Formally, one of the most frequently considered metric to evaluate this property is the so-called Intra-List Diversity (ILD) \abbrev{ILD}{Intra-List Diversity} \cite{2001SimilarityDiversity} which is defined as in Equation \ref{eq:diversity}

\begin{equation}
    \label{eq:diversity}
    ILD (\mathcal{R}_u) = \frac{1}{|\mathcal{R}_u||\mathcal{R}_u-1|} \sum_{i, j \in \mathcal{R}_u} dist(i,j)
\end{equation} where $dist(i,j)$ is a configurable distance measure between items. Such distance is generally a function of item features \cite{10.1145/1060745.1060754} or the distance in terms of interaction patterns by users \cite{2014NoveltyDiversityEnhancement}.


\subsection{Novelty}

Exposing users to a relevant experience that they would not have found by themselves is a desirable feature in RS, since obvious yet accurate recommendations do not suffice. To assess the level of such exposure, novelty is generally considered, being a piece of information that refers to how different it is with respect to what users have seen in the past \cite{2011RankNoveltyDiversity}. 

Many definitions of novelty have been proposed in the literature, but as a time component is at the core of this property, temporal novelty is usually a common approach that is defined as in Equation \ref{eq:novelty}

\begin{equation}
\label{eq:novelty}
    TN(\mathcal{R}^t_u) = \frac{|\mathcal{R}^t_u \setminus \cup_{\tau < t} \mathcal{R}^t_u|}{|\mathcal{R}^t_u|}
\end{equation} where $\mathcal{R}^t_u$ is the set of recommended items for user $u$ at time $t$. Having such definition may serve as a guideline for improving users' retention as studies show that their perception towards recommendation lists degrades over they if they do not receive novelty over past recommendations. This becomes more latent with users who have fewer interactions, who tend to receive less novel recommendations.

Although being generally understood as the difference between present and past experience, other types of novelty perspectives can be considered. Long tail novelty, for instance, considers how novel are items based on their popularity, which is specially used to evaluate whether RS are biased towards obvious recommendations of very popular items \cite{2014NoveltyDiversityEnhancement}.

\subsection{Serendipity}

Notice that in the definitions of novelty so far, no restriction on item relevance was made. To combine both unexpectedness with a positive response, the notion of serendipity is applied as particularization of novelty. Formally, it can be defined as in Equation 

\begin{equation}
    \label{eq:serendipity}
    Srdp(\mathcal{R}_u) = \frac{|(\mathcal{R}_u \setminus PM) \cap Rel_u|}{|\mathcal{R}_u|}
\end{equation} where $PM$ is a set of items predicted by a primitive method such as already seen items or most popular items and $Rel_u$ is a set of items that the user $u$ finds relevant \cite{2014NoveltyDiversityEnhancement}.

%\subsection{Utility}


